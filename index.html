<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  

  
  <title>Univesre</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Computer Fake Science">
<meta name="keywords" content="Univesre">
<meta property="og:type" content="website">
<meta property="og:title" content="Univesre">
<meta property="og:url" content="https://tjipot.github.io/index.html">
<meta property="og:site_name" content="Univesre">
<meta property="og:description" content="Computer Fake Science">
<meta property="og:locale" content="Chinese">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Univesre">
<meta name="twitter:description" content="Computer Fake Science">
  
    <link rel="alternate" href="/atom.xml" title="Univesre" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Univesre</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Universe Is Not Univesre</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://tjipot.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-札记-TensorFlow-机制" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/06/21/札记-TensorFlow-机制/" class="article-date">
  <time datetime="2019-06-20T16:38:06.000Z" itemprop="datePublished">2019-06-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/06/21/札记-TensorFlow-机制/">札记_TensorFlow_机制</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><em>1.参考<a href="https://www.cnblogs.com/bonelee/p/7902705.html" target="_blank" rel="noopener">CNBLOGS博文</a></em></p>
<p><em>2.参考<a href="http://www.tensorfly.cn/tfdoc/get_started/basic_usage.html" target="_blank" rel="noopener">TENSORFLY社区</a></em></p>
<h2 id="0-简介"><a href="#0-简介" class="headerlink" title="0.简介"></a>0.简介</h2><p>此札记的目的还是, 通过输出: 1.理解概念, 2.加深印象, 3.融会贯通.</p>
<p>首先, 先看一段Python中Tensorflow的代码:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># NumPy生成假数据(phony data), 共100个;</span></span><br><span class="line">x_data = np.float32(np.random.rand(<span class="number">2</span>, <span class="number">100</span>))         <span class="comment"># 随机生成: shape=(2,100);</span></span><br><span class="line">y_data = np.dot([<span class="number">0.100</span>, <span class="number">0.200</span>], x_data) + <span class="number">0.300</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造一个线性模型;</span></span><br><span class="line"><span class="comment"># 只有tensor(变量)才能被tf的图接受(这是语法上的问题);</span></span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">1</span>]))</span><br><span class="line"><span class="comment"># 均匀分布的介于-1和1之间的1行2列的tensor(array);</span></span><br><span class="line">W = tf.Variable(tf.random_uniform([<span class="number">1</span>, <span class="number">2</span>], <span class="number">-1.0</span>, <span class="number">1.0</span>))</span><br><span class="line">y = tf.matmul(W, x_data) + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最小化方差;</span></span><br><span class="line">loss = tf.reduce_mean(tf.square(y - y_data))        <span class="comment"># 图中的一个operator;</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>)  <span class="comment"># 图中的一个operator;</span></span><br><span class="line">train = optimizer.minimize(loss)                    <span class="comment"># 图中的一个operator;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化变量;</span></span><br><span class="line">init = tf.initialize_all_variables()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动图(graph);</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拟合平面;</span></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> xrange(<span class="number">0</span>, <span class="number">201</span>):</span><br><span class="line">    <span class="comment"># train是一个operator, 图(即sess)的run()会自动根据依赖, 进行相应计算;</span></span><br><span class="line">    sess.run(train)</span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">print</span> step, sess.run(W), sess.run(b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到最佳拟合结果 W: [[0.100  0.200]], b: [0.300]</span></span><br></pre></td></tr></table></figure>

<p>在上面的注释中, 我反复添加了注释<figure class="highlight plain"><figcaption><span>图中的一个operator;```, 是为了强调这三行代码中的每个Python变量都是tf图中的operator. 这里的疑问是: 明摆着的是Python代码, 可底层到底如何运作的? 难道Python解释器自动感知了那三个变量? 它怎么知道是```train```和```optimizer```变量是要做模型的训练和优化? 它并没有被告知使用什么模型?</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">上面的例子直接使用了复杂的训练和优化流程来解释tf图的概念, 下面用一个简单点的例子, 同样也是tf图的概念, 只不过数据没那么复杂了:</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"># 创建一个变量, 初始化为标量 0.</span><br><span class="line">state     = tf.Variable(0, name=&quot;counter&quot;)</span><br><span class="line"># 创建一个 op, 其作用是使 state 增加 1</span><br><span class="line">one       = tf.constant(1)</span><br><span class="line">new_value = tf.add(state, one)</span><br><span class="line">update    = tf.assign(state, new_value)</span><br><span class="line"># 启动图后, 变量必须先经过`初始化` (init) op 初始化,</span><br><span class="line"># 首先必须增加一个`初始化` op 到图中.</span><br><span class="line">init_op   = tf.initialize_all_variables()</span><br><span class="line"></span><br><span class="line"># 启动图, 运行 op</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">  sess.run(init_op)     # 运行 &apos;init&apos; op</span><br><span class="line">  print sess.run(state) # 打印 &apos;state&apos; 的初始值</span><br><span class="line">  for _ in range(3):    # 运行 op, 更新 &apos;state&apos;, 并打印 &apos;state&apos;</span><br><span class="line">    sess.run(update)</span><br><span class="line">    print sess.run(state)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出为:</span></span><br><span class="line"><span class="comment"># 0</span></span><br><span class="line"><span class="comment"># 1</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="comment"># 3</span></span><br></pre></td></tr></table></figure>

<br>

<h2 id="1-基本使用"><a href="#1-基本使用" class="headerlink" title="1.基本使用"></a>1.基本使用</h2><hr>
<p>好了, 尽管有上面的例子, 但它们只是代码(没从中看出tf图的样子), 下面, 开始解释代码背后的tf图逻辑(不准确解释为”编译树”, 这才是tf图, 眼睛看不见只有脑子能”看见”的那种图).</p>
<p>在Tensorflow中:</p>
<ul>
<li><p>使用<strong>图(graph)</strong>来表示计算任务;</p>
<p>  (p.s. 说到图, 就隐含背后计算的依赖)</p>
</li>
<li><p>在被称之为<strong>会话(Session)</strong>的上下文(context)中执行图;</p>
<p>  (p.s. 图中的数学计算当然要消耗计算资源, 而Python中的Session便是管理这种上下文(1.进入Session代码块:使用资源, 2.执行别的代码块:结束Session代码块中对OS资源的占用)的, 故要在Session中执行图的计算)</p>
</li>
<li><p>使用<strong>tensor</strong>表示数据;</p>
<p>  (p.s. 不必紧张, tensor根本不是能用于真正计算的数据类型(比如<figure class="highlight plain"><figcaption><span>```float```等才是), 它只是一种数据类别的强制要求(Google说在我的tf中必须要用tensor来包装一切数据, 就当这样便于管理tf图吧), 这个关系好比Class与Class中基本数据类型的关系一样, 只不过这里是tf, 那里是一般编程语言的Class概念).</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">+ 通过**变量(Variable)**维护状态;</span><br><span class="line">+ 使用**feed**和**fetch**可以为任意的操作(arbitrary operation)赋值或者从其中获取数据.</span><br><span class="line">&lt;br&gt;&lt;br&gt;</span><br><span class="line"></span><br><span class="line">## 2.综述</span><br><span class="line">Tensorflow是一个编程系统, 使用图来表示计算任务, 图中的节点被称之为**op**(operation的缩写, 意为具体某种计算操作, 如加法), 如下图中的&apos;绿/红/黄/蓝色&apos;节点. 一个op获得0个或多个Tensor数据, 执行计算, 产生0个或多个Tensor结果. 每个Tensor是一个类型化的多维数组, 例如, 你可以将一小组图像集表示为一个四维浮点数数组([图像个数x图像的三维]), 这四个维度分别是[batch, height, width, channels].</span><br><span class="line"></span><br><span class="line">![ASimpleTFGraph.png](https://i.loli.net/2019/06/19/5d09bbd46ddfc97012.png)</span><br><span class="line">*图自[SRC](http://www.sohu.com/a/203650826_314987).*</span><br><span class="line"></span><br><span class="line">一个tf图描述了计算的过程(这个过程即计算的依赖关系). 为了进行计算, 图必须在&apos;会话&apos;里被启动(在Session代码块中进行). 会话将图的op分发到诸如CPU或GPU之类的设备上, 同时提供执行op的方法(e.g. 具体操作, 如加法, 还是乘法, 还是卷积, etc.). 这些方法执行后, 将产生的tensor结果返回. 在Python中, tensor结果是```numpy```的```ndarray```对象; 在C和C++语言中, tensor结果是```tensorflow::Tensor```实例对象.</span><br><span class="line">&lt;br&gt;&lt;br&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 3.计算图</span><br><span class="line">Tensorflow的程序通常被组织成: 一个**1.构建阶段**和一个**2.执行阶段**(我理解成**定义**和**使用**tf图). </span><br><span class="line"></span><br><span class="line">在构建阶段, op的执行步骤被描述成一个tf图. 在执行阶段, 使用会话执行tf图中的op. **例如**, 通常在构建阶段创建一个图来表示和训练神经网络, 然后在执行阶段反复执行图中的训练op(即```train```变量).</span><br><span class="line"></span><br><span class="line">(p.s. TensorFlow支持C, C++, Python编程语言. 目前, TensorFlow的Python库更加易用, Python库提供了大量的辅助函数来简化构建图的工作, 但在C和C++中, 它们尚未被支持, 但三种语言的会话库(session libraries)是一致的)</span><br><span class="line">&lt;br&gt;&lt;br&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 4.构建图(&apos;变量声明&apos;与&apos;依赖&apos;)</span><br><span class="line"></span><br><span class="line">举例来说, 构建图(如构建AlexNet的tf图), 就是在Python的TF代码中定义好变量的调用关系(如池化层的函数将卷积层的变量当做输入, 这就是一个调用关系, 即图的依赖), 这个AlexNet的输入, 5个Conv层和3个FC层的依赖关系, 就体现在Python的TF代码的变量调用关系中(而继续深究下去, 可能要查找&apos;编译树&apos;了).</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">构建图的第一步, 是创建&apos;**源op**&apos;(source op). 源op不需要任何输入, 例如常量(Constant), 它的输出被传递给其它op做进一步的运算. 在代码中(如下), op构造器的返回值代表被该op的输出, 这些返回值可以传递给其它op构造器作为输入(返回值=输出, 这不是废话吗..).</span><br><span class="line"></span><br><span class="line">TF的Python库有一个默认图(default graph), op构造器可以为其增加节点, 这个默认图对许多程序来说已经足够用了(代码中并没有相应代码语句的表示, 只不过由Python解释器默认使用). (阅读&apos;Graph类&apos;的文档来了解如何管理多个图)</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line"># 创建一个常量op, 产生一个1x2矩阵, 这个 op 被作为一个节点加到默认图中.</span><br><span class="line"># 构造器(指&apos;tf.constant()&apos;)的返回值代表该常量op的返回值.</span><br><span class="line">matrix1 = tf.constant([[3., 3.]])</span><br><span class="line"></span><br><span class="line"># 创建另外一个常量op, 生成一个2x1矩阵.</span><br><span class="line">matrix2 = tf.constant([[2.],[2.]])</span><br><span class="line"></span><br><span class="line"># 创建一个&apos;矩阵乘法matmul&apos;op, 把&apos;matrix1&apos;和&apos;matrix2&apos;作为输入.</span><br><span class="line"># 返回值&apos;product&apos;代表矩阵乘法的结果.</span><br><span class="line">product = tf.matmul(matrix1, matrix2)</span><br><span class="line"></span><br><span class="line"># 至此, 变量的调用关系, 也即tf图(依赖)的构建, 完成了;</span><br></pre></td></tr></table></figure></p>
</li>
</ul>
<p>默认图现在有三个节点, 两个constant() op, 和一个matmul() op. 为了真正进行矩阵相乘运算, 并得到矩阵乘法的 结果, 你必须<strong>在会话里启动这个图</strong>. </p>
<p>(p.s. 这好像有点杀鸡用牛刀了: 矩阵相乘, 我不能用普通的Python代码?! Don’t worry, 这只是个举例, <strong>牛刀能杀鸡, 也能宰牛</strong>)<br><br></p>
<h2 id="5-启动图-在会话中启动"><a href="#5-启动图-在会话中启动" class="headerlink" title="5.启动图(在会话中启动)"></a>5.启动图(在会话中启动)</h2><p>构造阶段完成后, 才能启动图(p.s. 要不然没意义啊, 没构造完如何进行有意义的计算). 启动图的第一步是创建一个Session对象(这是语法机制), 如果无任何创建参数(传给<figure class="highlight plain"><figcaption><span>会话构造器将启动默认图. (欲了解完整的'会话API', 请阅读Session类)</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">```python</span><br><span class="line"># 启动默认图(无参数传入);</span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"># 1.调用sess的&apos;run()&apos;方法来执行&apos;矩阵乘法op&apos;, 传入&apos;product&apos;作为该方法的参数; </span><br><span class="line"># 2.在&apos;4节&apos;提到, &apos;product&apos;代表了&apos;矩阵乘法op&apos;的输出, </span><br><span class="line">#   传入它是向方法表明, 我们希望取回&apos;矩阵乘法op&apos;的输出;</span><br><span class="line"># 3.整个执行过程是自动化的, (!!!)会话负责传递op所需的全部输入(!!!), op通常是并发执行的(Mark: 如何并发?!);</span><br><span class="line"># 4.函数调用&apos;run(product)&apos;触发了图中三个op(两个常量op和一个矩阵乘法op)的执行(!!!);</span><br><span class="line"># 5.返回值&apos;result&apos;是一个numpy的`ndarray`对象; (tf的返回值都是ndarray对象)</span><br><span class="line">result = sess.run(product)  # (!!!)前面构建好了图, 才有这一步的启动和计算;</span><br><span class="line">print result  # ==&gt; [[ 12.]]</span><br><span class="line"></span><br><span class="line"># 任务完成, (需)关闭会话;</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure></p>
<p>Session对象在使用完后需要关闭以释放资源, 除了显式调用<figure class="highlight plain"><figcaption><span>也可以使用"with"代码块, 来自动完成(符合规范的程序的)关闭动作:</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">```python</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">  result = sess.run([product])</span><br><span class="line">  print result</span><br></pre></td></tr></table></figure></p>
<br>

<h4 id="关于CPU和GPU-等设备"><a href="#关于CPU和GPU-等设备" class="headerlink" title="关于CPU和GPU(等设备)"></a>关于CPU和GPU(等设备)</h4><p>在实现上, TF将图<del>形</del>定义转换成分布式执行的操作, 以<strong><em>充分利用</em></strong>可用的计算资源(如CPU或GPU). TF能自动检测是使用CPU还是GPU, 一般不需要人为显式指定, 如果检测到GPU, TF会尽可能地利用找到的第一个GPU来执行操作.</p>
<p>如果机器上有<strong><em>超过一个</em></strong>可用的GPU, 除第一个外的其它GPU默认不参与计算. 如果要让TF使用这些GPU, 必须将op明确指派给它们执行. <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">```python</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">  with tf.device(&quot;/gpu:1&quot;):</span><br><span class="line">    matrix1 = tf.constant([[3., 3.]])</span><br><span class="line">    matrix2 = tf.constant([[2.],[2.]])</span><br><span class="line">    product = tf.matmul(matrix1, matrix2)</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure></p>
<p>CPU和GPU等设备用字符串进行标识, 目前支持的设备包括:</p>
<ul>
<li>“<strong>/cpu:0</strong>“: 机器的CPU;</li>
<li>“<strong>/gpu:0</strong>“: 机器的第一个GPU, 如果有的话;</li>
<li>“<strong>/gpu:1</strong>“: 机器的第二个GPU, 以此类推.</li>
</ul>
<p>(阅读使用GPU章节, 了解 TensorFlow GPU 使用的更多信息)<br><br><br></p>
<h2 id="6-交互式使用"><a href="#6-交互式使用" class="headerlink" title="6.交互式使用"></a>6.交互式使用</h2><p>文档中的Python示例使用一个’会话Session’来启动图, 并调用<figure class="highlight plain"><figcaption><span>为了便于使用诸如'IPython'之类的 Python交互环境, 可以使用```InteractiveSession```代替```Session```类, 使用```Tensor.eval()```和```Operation.run()```方法代替```Session.run()```, 这样可以避免使用一个变量来持有会话.</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">(上面的话的意思是: 我就不```sess.run()```了, 你在代码中声明一下```InteractiveSession ```, 然后我就告诉Python解释器关于资源的使用方式, 交由你的```Tensor.eval()```和```Operation.run()```来使用, 但它(Python解释器)还是会管理的)</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"># 进入一个交互式 TensorFlow 会话.</span><br><span class="line">import tensorflow as tf</span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line"></span><br><span class="line">x = tf.Variable([1.0, 2.0])</span><br><span class="line">a = tf.constant([3.0, 3.0])</span><br><span class="line"># 使用初始化器 initializer op 的 run() 方法初始化 &apos;x&apos; </span><br><span class="line">x.initializer.run()</span><br><span class="line"># 增加一个&apos;减法sub op&apos;, 从&apos;x&apos;减去&apos;a&apos;. 运行&apos;减法op&apos;, 输出结果;</span><br><span class="line">sub = tf.sub(x, a)</span><br><span class="line">print sub.eval()  # 直接eval()一下好了, 就不要sess.run()了;</span><br><span class="line"># ==&gt; [-2. -1.]</span><br></pre></td></tr></table></figure></p>
<br>


<h2 id="7-Tensor"><a href="#7-Tensor" class="headerlink" title="7.Tensor"></a>7.Tensor</h2><p>TF程序使用’Tensor数据结构’来代表所有的数据, 计算图中, 操作间传递的数据<strong>都是</strong>Tensor(你可以把TF的Tensor看作是一个<strong>n维的数组或列表</strong>). 一个Tensor包含一个静态类型<strong><em>rank</em></strong>, 和一个<strong><em>shape</em></strong>. </p>
<p>(想了解TF是如何处理这些概念的, 参见Rank, Shape, 和Type)<br><br><br></p>
<h2 id="8-变量-More-Details"><a href="#8-变量-More-Details" class="headerlink" title="8.变量(More Details)"></a>8.变量(More Details)</h2><p><strong>变量维护图执行过程中的***</strong>状态信息***, 下面的例子演示了如何使用变量实现一个简单的计数器. (参见’变量’章节了解更多细节)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">###### 创建图 ######</span></span><br><span class="line"><span class="comment"># 1.创建一个变量, 初始化为标量0;</span></span><br><span class="line">state = tf.Variable(<span class="number">0</span>, name=<span class="string">"counter"</span>)</span><br><span class="line"><span class="comment"># 2.创建一个op, 其作用是使state增加1;</span></span><br><span class="line">one       = tf.constant(<span class="number">1</span>)</span><br><span class="line">new_value = tf.add(state, one)</span><br><span class="line">update    = tf.assign(state, new_value)</span><br><span class="line"></span><br><span class="line"><span class="comment">###### 启动图 ######</span></span><br><span class="line"><span class="comment"># 启动图后, #变量#必须先经过`初始化op`(init op)的初始化,</span></span><br><span class="line"><span class="comment"># 即, 必须增加一个`初始化op`到(默认)图中;</span></span><br><span class="line">init_op = tf.initialize_all_variables()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动图, 运行op;</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  <span class="comment"># 运行'init op';</span></span><br><span class="line">  sess.run(init_op)</span><br><span class="line">  <span class="comment"># 打印'state'的初始值;</span></span><br><span class="line">  <span class="keyword">print</span> sess.run(state)</span><br><span class="line">  <span class="comment"># 运行'state op', 更新'state', 并打印'state'值;</span></span><br><span class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">    sess.run(update)</span><br><span class="line">    <span class="keyword">print</span> sess.run(state)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出:</span></span><br><span class="line"><span class="comment"># 0</span></span><br><span class="line"><span class="comment"># 1</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="comment"># 3</span></span><br></pre></td></tr></table></figure>

<p>代码中<figure class="highlight plain"><figcaption><span>正如`add()`操作一样, 所以在调用`run()`执行表达式之前, 它并不会真正地执行(赋值操作). 通常会将一个统计模型中的参数**表示为一组变量**, 如, ***你可以将一个神经网络的权重作为某个变量存储在一个Tensor中***. 在训练过程中, 通过重复运行训练图, 更新这个 Tensor. (***原来如此!!!***)</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&lt;br&gt;&lt;br&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 9.Fetch(&apos;sess.run()的结果&apos;)</span><br><span class="line"></span><br><span class="line">为了取回&apos;op的输出内容&apos;, 可以在使用`Session`对象的`run()`调用执行图时, **&lt;del&gt;传入一些Tensor&lt;/del&gt;**(原文写得这么不明白?!)声明一个Tensor(即Python变量), 这些Tensor会帮助你&lt;del&gt;取回结果&lt;/del&gt;保存结果Tensor到等号左边的变量中. 在之前的例子里, 我们只取回了单个节点的&apos;state&apos;值, 但也可以取回多个Tensor(如下例):</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">input1   = tf.constant(3.0)</span><br><span class="line">input2   = tf.constant(2.0)</span><br><span class="line">input3   = tf.constant(5.0)</span><br><span class="line">intermed = tf.add(input2, input3)</span><br><span class="line">mul      = tf.mul(input1, intermed)</span><br><span class="line"></span><br><span class="line">with tf.Session():</span><br><span class="line">  result = sess.run([mul, intermed])</span><br><span class="line">  print result</span><br><span class="line">  # 输出(&apos;mul&apos;一个结果, &apos;intermed&apos;一个结果):</span><br><span class="line">  # [array([ 21.], dtype=float32), array([ 7.], dtype=float32)]</span><br></pre></td></tr></table></figure></p>
<p>(需要获取的多个Tensor值，在’result op’的一次运行中一起获得(而不是逐个去获取))<br><br><br></p>
<h2 id="10-Feed-即’Placeholder’"><a href="#10-Feed-即’Placeholder’" class="headerlink" title="10.Feed(即’Placeholder’)"></a>10.Feed(即’Placeholder’)</h2><p>上述示例在计算图中引入了Tensor, 它们以常量或变量的形式存储. TF还提供了’feed机制’, 该机制可以临时替代图中的任意操作中的Tensor, 可以对图中任何op提交补丁(补丁: Excuse Me?!), 在后续中插入具体的Tensor值. Feed使用一个Tensor值临时替换一个操作的输出结果. 你可以提供Feed数据作为<code>run()</code>调用的参数. Feed只在调用它的方法内有效, 方法结束, Feed就会消失. (讲得真绕, Feed之意就是在某处Feed数据给需要被Feed的东西)</p>
<p>最常见的用例是将某些特殊的操作指定为’feed’操作, 标记的方法是使用<code>tf.placeholder()</code>为这些操作创建占位符.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">input1 = tf.placeholder(tf.types.float32)  <span class="comment"># Feed;</span></span><br><span class="line">input2 = tf.placeholder(tf.types.float32)  <span class="comment"># Feed;</span></span><br><span class="line">output = tf.mul(input1, input2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  <span class="comment"># Feed: feed_dict=&#123;output op所需的数据(值)&#125;;</span></span><br><span class="line">  <span class="keyword">print</span> sess.run([output], feed_dict=&#123;input1:[<span class="number">7.</span>], input2:[<span class="number">2.</span>]&#125;)</span><br><span class="line">  <span class="comment"># 输出: [array([ 14.], dtype=float32)]</span></span><br></pre></td></tr></table></figure>

<p>(如果没有正确提供Feed, <code>placeholder()</code> op将会报错误. <a href="http://www.tensorfly.cn/tfdoc/tutorials/mnist_tf.html" target="_blank" rel="noopener">MNIST全连通Feed教程</a>(<a href="https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/g3doc/tutorials/mnist/fully_connected_feed.py" target="_blank" rel="noopener">source code</a>)给出了一个更大规模的使用Feed的例子)</p>
<br>
(Note *Ends*, Keep For Future *Fine-Tuning*, :))

      
    </div>
    <footer class="article-footer">
      <a data-url="https://tjipot.github.io/2019/06/21/札记-TensorFlow-机制/" data-id="cjx5fa5g500059nzsk4zr9c3i" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-札记-论文-AlexNet" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/06/21/札记-论文-AlexNet/" class="article-date">
  <time datetime="2019-06-20T16:37:49.000Z" itemprop="datePublished">2019-06-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/06/21/札记-论文-AlexNet/">札记_论文_AlexNet</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>AlexNet的”官方”论文为<strong><em>ImageNet Classiﬁcation with Deep Convolutional Neural Networks</em></strong> [<a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networ" target="_blank" rel="noopener">ref</a>], 发表于2012(7年前, :)), 论文的三位作者分别是Alex(第一作者), Ilya和Geoffrey Hinton(前两位的导师), 其中的<strong>Dropout</strong>概念, 应当是很有影响的概念了.</p>
<p>此篇论文共7个章节, 下面我将论文的读后理解札记下来, 输出出来加深印象.</p>
<p>(Abstract)</p>
<ol>
<li>Introduction</li>
<li>The Dataset</li>
<li>The Architecture</li>
<li>Reducing Overfitting</li>
<li>Details of learning</li>
<li>Results</li>
<li>Discussion</li>
</ol>
<p>(Reference)<br><br><br></p>
<hr>
<h4 id="Abstract"><a href="#Abstract" class="headerlink" title="(Abstract)"></a>(Abstract)</h4><p>概括了作者要提出的模型的一些简要情况: ILSVRC-2010的top-1和top-5的错误率为37.5%和17.0%的一个模型(比之前的都要好), 以及这个模型有6,000万个参数和6.5万个神经元. 该模型使用的一些技巧, 有: 5个卷积层(后接最大池化层)和3个全连接层(其中两个应用了Dropout), (为了加速训练)非饱和神经元(即ReLU)以及GPU的使用. 使用该模型的一个变化版本, 在ILSVRC-2012得到了15.3%的错误率(top-5).<br><br><br></p>
<hr>
<h4 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h4><p>该部分首先简述了小数据集(几万张的级别)的作用: 在简单的识别任务中(如MNIST数字识别), 这样的数据集可以适用和胜任. 然后, 说明了在真实环境中对象是会变化的(variability, 注: 如角度, 成像大小等), 引出我们需要更大的数据集的需求, 并提到了其中两个: LabelMe和ImageNet.</p>
<p>接着该部分的第二段, 提到了模型的学习能力(learning capacity, 学习什么?! 识别对象): 即便有了像ImageNet这样的大数据集, 由于对象识别任务的巨大复杂性(immense complexity, 没法靠人工去一一指定), 也不可能将对象识别这个任务明确指定好(具体如何识别对象). 继续, 文中提到CNN, 提及CNN的学习能力(匹配识别对象的复杂度)可由CNN模型的深度(depth)和广度(breadth)控制.</p>
<p>接下去的三段, 主要内容为: 1.CNN与GPU与数据, 2.论文的几点重要贡献罗列, 3.论文中网络的尺寸(受限于GPU内存与训练时间).<br><br><br></p>
<hr>
<h4 id="2-The-Dataset"><a href="#2-The-Dataset" class="headerlink" title="2.The Dataset"></a>2.The Dataset</h4><p>这一部分, 先是介绍了ImageNet(1,500万高分辨率图像, 22,000个类别), 而ILSVRC使用的是ImageNet的一个子集. 然后, 在下一段, 论文说明了测试集标签的问题: ILSVRC-2010是ILSVRC竞赛中唯一有测试集标签的版本(大多数实验都在此版本上进行), 论文中使用了top-1和top-5错误率作为报告的错误率.</p>
<p>最后一段, 讲述了论文处理图像分辨率的方式: 短边缩放至256的值, 并裁剪长边为256. 另外, 论文方法也对整个训练集的图片进行了”减去平均活跃值”(mean activity)的处理.<br><br><br></p>
<hr>
<h4 id="3-The-Architecture"><a href="#3-The-Architecture" class="headerlink" title="3.The Architecture"></a>3.The Architecture</h4><p>该部分主要讲述模型的架构(8个主要网络层 = 5个Conv + 3个FC), 以及网络中使用到的一些特性(ReLU, 多GPU训练等).</p>
<p>首先, 在3.1节ReLU Nonlinearity中, 论文对比了饱和非线性性(saturating nonlinearity, 如tanh函数)和非饱和非线性性(non-saturating nonlinearity, 此处的ReLU函数), 指出ReLU加快了网络的训练(图1).<br><img src="https://i.loli.net/2019/06/18/5d08e449496d242029.png" alt="AlexNet_Fig_01.png">{:height=”250”}</p>
<p>(图自: 论文 Figure 1)</p>
<p>其次, 在3.2节”Training on Multiple GPUs”中, 提到受限于GPU内存的大小(论文使用的GPU内存为3GB), 故将网络分布在两个GPU上进行训练, 此并行方案是: 基本上, 每个GPU放置一半的核; 另外, 在第3层时进行GPU间的通信: 第3层的核(分别在两个GPU中)会将第2层的所有feature map(两个GPU中所有的)同时作为输入进行卷积运算.</p>
<p>接下去, 论文的3.3节”Local Response Normalization”和3.4节”Overlapping Pooling”分别讲述了这两种技术(局部响应归一化, 重叠池化), 但现在这两种技巧并不被经常使用(?!).</p>
<p>最后, 3.5节”Overall Architecture”描述了AlexNet模型的总体架构, 为5个卷积层与3个全连接层的”一个序列”, 具体有如下图的结构:<br><img src="https://i.loli.net/2019/06/18/5d08e44a99ba510264.png" alt="AlexNet_Fig_02.png"></p>
<p>(图自: 论文 Figure 2)</p>
<p><br><br></p>
<hr>
<h4 id="4-Reducing-Overfitting"><a href="#4-Reducing-Overfitting" class="headerlink" title="4.Reducing Overfitting"></a>4.Reducing Overfitting</h4><p>在这一部分, 论文提及到了减小过拟合的两个主要方式(数据增强, Dropout), 这么做的原因是因为网络架构的参数有6,000万个(很多, 多于数据集的scale), 会产生过拟合.</p>
<p>在4.1节”Data Augmentation”中, 论文提及了两种数据增强的方式: 1.图像变换和水平翻转(裁剪出224x224的图像块), 2.改变训练集图像RGB通道的强度(对像素值集合执行PCA并对图像加上多倍找到的主成分).</p>
<p>在4.2节”Dropout”中, 论文介绍了这一概念, 以及它的应用位置: 在三个全连接层的前两层的输出中应用.<br><br><br></p>
<hr>
<h4 id="5-Details-of-learning"><a href="#5-Details-of-learning" class="headerlink" title="5.Details of learning"></a>5.Details of learning</h4><p>此部分具体讲述了论文模型的训练细节: 如参数设置, 参数初始化等. 在模型训练中, 论文团队使用了SGD(随机梯度下降, batch_size=128), 动量(=0.9), 权重衰减(=0.0005)等设置.<br><br><br></p>
<hr>
<h4 id="6-Results"><a href="#6-Results" class="headerlink" title="6.Results"></a>6.Results</h4><p>论文的神经网络在ILSVRC-2010数据集上的结果有top-1 37.5%和top-5 17.5%的表现, 这在当时(2012年及以前)来说, 是已公布结果中最好的(ILSVRC-2010竞赛的最佳结果是: top-1 47.1%, top-5 28.2%). 另外, 论文也涵盖了所涉模型在ILSVRC-2012竞赛(测试集标签不公开)中的结果.<br><br><br></p>
<hr>
<h4 id="7-Discussion"><a href="#7-Discussion" class="headerlink" title="7.Discussion"></a>7.Discussion</h4><p>讨论部分, 论文总述大型深度卷积神经网络(A large, deep convolutional neural network)可以取得的破纪录结果, 并佐证深度的重要性: 在论文的模型中, 移除任何中间层都会导致大约2%的top-1性能损失.</p>
<p>最后, 论文进行了一些模型之外的探讨.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://tjipot.github.io/2019/06/21/札记-论文-AlexNet/" data-id="cjx5fa5g400049nzsmstowpqc" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-札记-PYTHON" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/06/21/札记-PYTHON/" class="article-date">
  <time datetime="2019-06-20T16:35:42.000Z" itemprop="datePublished">2019-06-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/06/21/札记-PYTHON/">札记_PYTHON</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><em>1.参考<a href="https://gitbook.cn/gitchat/column/5ad56a79af8f2f35290f6535#catalog" target="_blank" rel="noopener">博文</a></em></p>
<p>　　现有最基本最基本的Python概念点(看一遍都懂, 就记不住)整理了, Mark在文章中, 日后有更丰富的经验, 可以以此为基础添加上去, 使枝叶丰茂. (P.s. 重要的是, 摆脱Java和C++等编译语言的长期影响!)<br><br>
　　</p>
<hr>
<h3 id="Section-00-Python简史-略"><a href="#Section-00-Python简史-略" class="headerlink" title="Section 00: Python简史(略)"></a>Section 00: Python简史(略)</h3><br>

<hr>
<h3 id="Section-01-预备知识-开发环境-略"><a href="#Section-01-预备知识-开发环境-略" class="headerlink" title="Section 01: 预备知识, 开发环境(略)"></a>Section 01: 预备知识, 开发环境(略)</h3><br>

<hr>
<h3 id="Section-02-数字-运算符"><a href="#Section-02-数字-运算符" class="headerlink" title="Section 02: 数字, 运算符"></a>Section 02: 数字, 运算符</h3><h4 id="数字"><a href="#数字" class="headerlink" title="数字:"></a>数字:</h4><p>Python中有三种数字: int, float, complex;</p>
<h4 id="运算符"><a href="#运算符" class="headerlink" title="运算符:"></a>运算符:</h4><p><strong>1</strong>.算术(7种): 加(<figure class="highlight plain"><figcaption><span>减(```-```), 乘(```*```), 除(```/```), 取模(```%```), 幂运算(```**```)和取整预算(```//```);</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">**2**.比较(6种, 结果是布尔值): ```&gt;```(大于), ```&lt;```(小于), ```==```(等于), ```!=```(不等于), ```&gt;=```(大于等于), ```&lt;=```(小于等于); </span><br><span class="line"></span><br><span class="line">**3**.赋值运算(&quot;=&quot;与算术运算符搭配使用): ```+=```, ```-=```, ```*=```, ```/=```, ```%=```, ```**=```, ```//=```;</span><br><span class="line"></span><br><span class="line">**4**.逻辑运算: ```and or not```(与或非);</span><br><span class="line"></span><br><span class="line">**5**.位运算(操作的是二进制上的bit): ```&amp;```与, ```|```或, ```^```异或, ```~```取反, ```&gt;&gt;```右移(几位), ```&lt;&lt;```左移(几位, 高位丢弃, 低位补0);</span><br><span class="line"></span><br><span class="line">**6**.成员运算: 某元素是否在一个数据结构(如字符串,列表,元组,字典)中, 关键字```in```;</span><br><span class="line"></span><br><span class="line">**7**.身份运算符: 比较两个标识符所引用对象的存储单元, 判断它们是否引用自同一个对象, 关键字```is```和```is not```;</span><br><span class="line">&lt;br&gt;</span><br><span class="line">&lt;br&gt;</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">### Section 03: 字符串, 列表, 栈</span><br><span class="line">数据结构(包含的是数据元素): 包括字符串, 列表, 元组, 字典, 栈, 队列和集合; 很多时候, 确定了数据结构后, 算法就容易得到了.</span><br><span class="line"></span><br><span class="line">**1**.字符串: 注意(很多)内建方法;</span><br><span class="line"></span><br><span class="line">**2**.列表: 最基本最重要的数据结构, 访问列表中的元素可以是以&quot;切片&quot;的形式, 列表的迭代, 则是熟悉的&quot;for ele in list:&quot;的&quot;for循环&quot;形式;</span><br><span class="line"></span><br><span class="line">**3**.栈: 列表的两个内建函数的组合: ```append()```和```pop()```, 即列尾添加和列尾取出(先进后出);</span><br><span class="line">&lt;br&gt;</span><br><span class="line">&lt;br&gt;</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">### Section 04: (数据类型)元组, 字典, 集合, 队列</span><br><span class="line"></span><br><span class="line">**1**.元组, 属于序列类别;</span><br><span class="line"></span><br><span class="line">**2**.字典(可理解成&apos;映射表&apos;), 操作: 创建, 添加, 修改, 删除</span><br><span class="line"></span><br><span class="line">**3**.集合: 一个或多个确定的元素(仅元素)所构成的整体, 基本功能包括关系测试和消除重复元素;</span><br><span class="line">集合操作: 创建, 添加, 移除元素;</span><br><span class="line">集合运算: 交/并/补集;</span><br><span class="line"></span><br><span class="line">**4**.队列: &quot;先进先出&quot;, 就好像排队打饭, Python中有专门的队列模块Queue(以及deque, etc.); </span><br><span class="line">```python</span><br><span class="line">from collections import deque</span><br><span class="line">queue = deque([&apos;A&apos;,&apos;B&apos;,&apos;C&apos;])</span><br><span class="line"># 操作;</span><br><span class="line">queue.append(&apos;D&apos;)</span><br><span class="line">queue.popleft()</span><br></pre></td></tr></table></figure></p>
<br>


<hr>
<h3 id="Section-05-条件-循环-其它语句"><a href="#Section-05-条件-循环-其它语句" class="headerlink" title="Section 05: 条件, 循环, 其它语句;"></a>Section 05: 条件, 循环, 其它语句;</h3><br>

<hr>
<h3 id="Section-06-Python抽象-gt-函数-对一组需求进行囊括-他们说抽象"><a href="#Section-06-Python抽象-gt-函数-对一组需求进行囊括-他们说抽象" class="headerlink" title="Section 06: Python抽象 -&gt; 函数, 对一组需求进行囊括(他们说抽象)"></a>Section 06: Python抽象 -&gt; 函数, 对一组需求进行囊括(他们说抽象)</h3><p>1.定义与调用, 2.参数传递(变量与引用/不可变/可变类型), 3.函数的参数类型(必须参数, 关键字参数, 默认参数和不定长参数), 4.Return语句(变量作用域: Local&gt;Enclosing&gt;Global&gt;Build-in, <a href="https://www.jianshu.com/p/3bb277c2935c" target="_blank" rel="noopener">详见</a>)
<br><br><br></p>
<hr>
<h3 id="Section-07-抽象-gt-类-略"><a href="#Section-07-抽象-gt-类-略" class="headerlink" title="Section 07: 抽象 -&gt; 类(略)"></a>Section 07: 抽象 -&gt; 类(略)</h3><br>

<hr>
<h3 id="Section-08-错误和异常-略"><a href="#Section-08-错误和异常-略" class="headerlink" title="Section 08: 错误和异常(略)"></a>Section 08: 错误和异常(略)</h3><br>



<hr>
<h3 id="Section-09-模块和标准库"><a href="#Section-09-模块和标准库" class="headerlink" title="Section 09: 模块和标准库"></a>Section 09: 模块和标准库</h3><p>1.Import语句与模块, 2.编写一个极简的模块, 3.模块的位置, 4.from…import语句, 5.深入模块, 6.用help获取帮助, 7.文档<code>__doc__</code>, 8.标准库;<br><br><br><br></p>
<hr>
<h3 id="Section-10-文件和流"><a href="#Section-10-文件和流" class="headerlink" title="Section 10: 文件和流"></a>Section 10: 文件和流</h3><p>一些场景下, 程序需要和外部进行交互, 如: 1.input和print函数等标准I/O类的交互, 2.此处的新的交互方式: 文件和流(读/写);<br>1.打开文件, 2.文件模式, 3.缓冲, 4.读写文件, 5.读写行, 6.使用fileinput进行迭代, 7.文件迭代器;<br><br><br><br></p>
<hr>
<h3 id="Section-11-数据库与网络编程-略"><a href="#Section-11-数据库与网络编程-略" class="headerlink" title="Section 11: 数据库与网络编程(略)"></a>Section 11: 数据库与网络编程(略)</h3><p>1.数据库支持, 2.网络编程(如自带的Socket模块);<br><br><br><br></p>
<hr>
<h3 id="Section-12-图形用户界面-略"><a href="#Section-12-图形用户界面-略" class="headerlink" title="Section 12: 图形用户界面(略)"></a>Section 12: 图形用户界面(略)</h3><p>(自带的: TKinter), Py不用来做图形界面开发, GUI工具包”很一般”;<br><br><br><br></p>
<hr>
<h3 id="Section-13-抢票软件-略"><a href="#Section-13-抢票软件-略" class="headerlink" title="Section 13: 抢票软件(略)"></a>Section 13: 抢票软件(略)</h3><br>
<br>

<hr>
<h3 id="Section-14-爬虫-略"><a href="#Section-14-爬虫-略" class="headerlink" title="Section 14: 爬虫(略)"></a>Section 14: 爬虫(略)</h3>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://tjipot.github.io/2019/06/21/札记-PYTHON/" data-id="cjx5fa5fv00009nzsqweue1ac" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-New-For-Hexo" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/06/21/New-For-Hexo/" class="article-date">
  <time datetime="2019-06-20T16:30:38.000Z" itemprop="datePublished">2019-06-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/06/21/New-For-Hexo/">New_For_Hexo</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="https://tjipot.github.io/2019/06/21/New-For-Hexo/" data-id="cjx5fa5g000029nzs5l5j1bcq" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/06/20/hello-world/" class="article-date">
  <time datetime="2019-06-20T15:44:21.000Z" itemprop="datePublished">2019-06-20</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/06/20/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://tjipot.github.io/2019/06/20/hello-world/" data-id="cjx5fa5g200039nzsd062xcj1" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">June 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/06/21/札记-TensorFlow-机制/">札记_TensorFlow_机制</a>
          </li>
        
          <li>
            <a href="/2019/06/21/札记-论文-AlexNet/">札记_论文_AlexNet</a>
          </li>
        
          <li>
            <a href="/2019/06/21/札记-PYTHON/">札记_PYTHON</a>
          </li>
        
          <li>
            <a href="/2019/06/21/New-For-Hexo/">New_For_Hexo</a>
          </li>
        
          <li>
            <a href="/2019/06/20/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 Haoran Ye<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>